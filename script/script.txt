So now we are going to put this connection to use.
Remember, we are trying to understand the Fourier transform relative to differential equations.
Let us consider the simplest case.

We will start our exploration with ordinary vectors, meaning the usual arrows that spring to mind.
We also switch to more suggestive symbols for this context.
Now, x and b are vectors.
And the linear operator D is not the differentiation operator but some linear transformation.
We can think about it as a matrix.
For our purposes, we will assume that D is a symmetric matrix or linear operator.
Shortly, we will see why.
It is still a linear operator, so the conclusions we will reach regarding operators, in general, will still be valid when we look at the differentiation operator.

Looking at this equation, we can think to ourselves, in an ideal world, what would be the slightest effect D could have on a vector.
Perhaps one of the least complicated effects it could have is simply multiplying a vector by a constant.
Obviously, this can't be true for every vector, so we are asking for the vectors that don't change direction after applying the operator D.
This is the well-known eigenvalue problem, and the pairs of vectors and constants found are known as the eigenvectors and eigenvalues of the operator.
Because we decided that D is symmetric, the eigenvectors will be orthogonal, and the eigenvalues will be real values.

Now, we can express any vector using the eigenvectors as a basis.
This ca n be done by projecting the vector using the inner product in each direction-
Summing the contributions in each direction, we get our vector back.
And why is this relevant?

Let's see what happens when we use this on the equation.
Moving D to right we must now consider its transpose.
Using the fact that D is symmetric, we now can use the results coming from the eigenvalue problem.
We can then move the constants inside the inner product outside.
And since this equation must be true for both eigenvectors in the sum, we find two equations.
Finally, we can rewrite the equation like so, yielding the components of the unknown vector.
And just like that, we don't have any more D's in our problem.
The problem has become an algebraic system of equations.
So we can now easily find the components of the unknown vector x.

What is the geometric meaning of this?
Using our original description of the problem the vectors are describing using the two unit vectors e 1 and e 2.
If we choose the the eigenvectors of the linear transformation as basis vectors, the linear transformation is much simpler being just a contraction or extension in perpendicular directions.

So we can find x just by rescaling the component of b in each direction parallel to the eigenvectors.
Precisely the statement of the formula we just found.

Moving back to our original coordinate, system we can confirm that the vector x found is indeed the solution to our problem, i.e., we have found a vector x that is mapped to the vector b after applying the linear transformation D.
